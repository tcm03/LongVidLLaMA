{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 100,
  "global_step": 309,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003236245954692557,
      "grad_norm": 322.0,
      "learning_rate": 5.000000000000001e-07,
      "loss": 2.0156,
      "step": 1
    },
    {
      "epoch": 0.006472491909385114,
      "grad_norm": 364.0,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.25,
      "step": 2
    },
    {
      "epoch": 0.009708737864077669,
      "grad_norm": 332.0,
      "learning_rate": 1.5e-06,
      "loss": 4.0625,
      "step": 3
    },
    {
      "epoch": 0.012944983818770227,
      "grad_norm": 344.0,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.8906,
      "step": 4
    },
    {
      "epoch": 0.016181229773462782,
      "grad_norm": 362.0,
      "learning_rate": 2.5e-06,
      "loss": 3.4531,
      "step": 5
    },
    {
      "epoch": 0.019417475728155338,
      "grad_norm": 362.0,
      "learning_rate": 3e-06,
      "loss": 3.0,
      "step": 6
    },
    {
      "epoch": 0.022653721682847898,
      "grad_norm": 19.375,
      "learning_rate": 3.5e-06,
      "loss": 0.0574,
      "step": 7
    },
    {
      "epoch": 0.025889967637540454,
      "grad_norm": 298.0,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.6875,
      "step": 8
    },
    {
      "epoch": 0.02912621359223301,
      "grad_norm": 316.0,
      "learning_rate": 4.5e-06,
      "loss": 2.1406,
      "step": 9
    },
    {
      "epoch": 0.032362459546925564,
      "grad_norm": 318.0,
      "learning_rate": 5e-06,
      "loss": 2.4062,
      "step": 10
    },
    {
      "epoch": 0.03559870550161812,
      "grad_norm": 310.0,
      "learning_rate": 4.999862004988709e-06,
      "loss": 2.7031,
      "step": 11
    },
    {
      "epoch": 0.038834951456310676,
      "grad_norm": 350.0,
      "learning_rate": 4.9994480351889364e-06,
      "loss": 2.8438,
      "step": 12
    },
    {
      "epoch": 0.042071197411003236,
      "grad_norm": 374.0,
      "learning_rate": 4.998758136301295e-06,
      "loss": 3.8438,
      "step": 13
    },
    {
      "epoch": 0.045307443365695796,
      "grad_norm": 312.0,
      "learning_rate": 4.997792384487867e-06,
      "loss": 2.4688,
      "step": 14
    },
    {
      "epoch": 0.04854368932038835,
      "grad_norm": 318.0,
      "learning_rate": 4.996550886363801e-06,
      "loss": 3.75,
      "step": 15
    },
    {
      "epoch": 0.05177993527508091,
      "grad_norm": 308.0,
      "learning_rate": 4.995033778985534e-06,
      "loss": 2.5625,
      "step": 16
    },
    {
      "epoch": 0.05501618122977346,
      "grad_norm": 342.0,
      "learning_rate": 4.993241229835666e-06,
      "loss": 2.4531,
      "step": 17
    },
    {
      "epoch": 0.05825242718446602,
      "grad_norm": 310.0,
      "learning_rate": 4.991173436804468e-06,
      "loss": 2.8281,
      "step": 18
    },
    {
      "epoch": 0.061488673139158574,
      "grad_norm": 350.0,
      "learning_rate": 4.9888306281680405e-06,
      "loss": 3.0312,
      "step": 19
    },
    {
      "epoch": 0.06472491909385113,
      "grad_norm": 284.0,
      "learning_rate": 4.986213062563104e-06,
      "loss": 1.8281,
      "step": 20
    },
    {
      "epoch": 0.06796116504854369,
      "grad_norm": 300.0,
      "learning_rate": 4.9833210289584574e-06,
      "loss": 2.3906,
      "step": 21
    },
    {
      "epoch": 0.07119741100323625,
      "grad_norm": 312.0,
      "learning_rate": 4.980154846623067e-06,
      "loss": 3.0,
      "step": 22
    },
    {
      "epoch": 0.0744336569579288,
      "grad_norm": 372.0,
      "learning_rate": 4.976714865090827e-06,
      "loss": 3.7656,
      "step": 23
    },
    {
      "epoch": 0.07766990291262135,
      "grad_norm": 356.0,
      "learning_rate": 4.97300146412197e-06,
      "loss": 2.7344,
      "step": 24
    },
    {
      "epoch": 0.08090614886731391,
      "grad_norm": 294.0,
      "learning_rate": 4.969015053661142e-06,
      "loss": 2.1719,
      "step": 25
    },
    {
      "epoch": 0.08414239482200647,
      "grad_norm": 300.0,
      "learning_rate": 4.964756073792148e-06,
      "loss": 2.5469,
      "step": 26
    },
    {
      "epoch": 0.08737864077669903,
      "grad_norm": 330.0,
      "learning_rate": 4.960224994689371e-06,
      "loss": 3.2812,
      "step": 27
    },
    {
      "epoch": 0.09061488673139159,
      "grad_norm": 270.0,
      "learning_rate": 4.955422316565856e-06,
      "loss": 1.6172,
      "step": 28
    },
    {
      "epoch": 0.09385113268608414,
      "grad_norm": 286.0,
      "learning_rate": 4.950348569618105e-06,
      "loss": 1.8984,
      "step": 29
    },
    {
      "epoch": 0.0970873786407767,
      "grad_norm": 322.0,
      "learning_rate": 4.9450043139675284e-06,
      "loss": 3.625,
      "step": 30
    },
    {
      "epoch": 0.10032362459546926,
      "grad_norm": 332.0,
      "learning_rate": 4.939390139598623e-06,
      "loss": 3.6562,
      "step": 31
    },
    {
      "epoch": 0.10355987055016182,
      "grad_norm": 264.0,
      "learning_rate": 4.933506666293834e-06,
      "loss": 1.6719,
      "step": 32
    },
    {
      "epoch": 0.10679611650485436,
      "grad_norm": 26.5,
      "learning_rate": 4.927354543565131e-06,
      "loss": 0.0806,
      "step": 33
    },
    {
      "epoch": 0.11003236245954692,
      "grad_norm": 304.0,
      "learning_rate": 4.920934450582311e-06,
      "loss": 2.7656,
      "step": 34
    },
    {
      "epoch": 0.11326860841423948,
      "grad_norm": 19.125,
      "learning_rate": 4.914247096098019e-06,
      "loss": 0.0586,
      "step": 35
    },
    {
      "epoch": 0.11650485436893204,
      "grad_norm": 306.0,
      "learning_rate": 4.907293218369499e-06,
      "loss": 2.6406,
      "step": 36
    },
    {
      "epoch": 0.11974110032362459,
      "grad_norm": 290.0,
      "learning_rate": 4.9000735850771e-06,
      "loss": 2.0156,
      "step": 37
    },
    {
      "epoch": 0.12297734627831715,
      "grad_norm": 306.0,
      "learning_rate": 4.8925889932395246e-06,
      "loss": 2.4219,
      "step": 38
    },
    {
      "epoch": 0.1262135922330097,
      "grad_norm": 304.0,
      "learning_rate": 4.88484026912584e-06,
      "loss": 2.4375,
      "step": 39
    },
    {
      "epoch": 0.12944983818770225,
      "grad_norm": 310.0,
      "learning_rate": 4.876828268164264e-06,
      "loss": 2.8438,
      "step": 40
    },
    {
      "epoch": 0.13268608414239483,
      "grad_norm": 362.0,
      "learning_rate": 4.868553874847728e-06,
      "loss": 3.0781,
      "step": 41
    },
    {
      "epoch": 0.13592233009708737,
      "grad_norm": 344.0,
      "learning_rate": 4.86001800263623e-06,
      "loss": 3.6406,
      "step": 42
    },
    {
      "epoch": 0.13915857605177995,
      "grad_norm": 324.0,
      "learning_rate": 4.8512215938559955e-06,
      "loss": 2.3438,
      "step": 43
    },
    {
      "epoch": 0.1423948220064725,
      "grad_norm": 314.0,
      "learning_rate": 4.84216561959545e-06,
      "loss": 3.0156,
      "step": 44
    },
    {
      "epoch": 0.14563106796116504,
      "grad_norm": 344.0,
      "learning_rate": 4.832851079598007e-06,
      "loss": 3.6094,
      "step": 45
    },
    {
      "epoch": 0.1488673139158576,
      "grad_norm": 340.0,
      "learning_rate": 4.8232790021517094e-06,
      "loss": 2.7344,
      "step": 46
    },
    {
      "epoch": 0.15210355987055016,
      "grad_norm": 43.0,
      "learning_rate": 4.813450443975705e-06,
      "loss": 0.1328,
      "step": 47
    },
    {
      "epoch": 0.1553398058252427,
      "grad_norm": 328.0,
      "learning_rate": 4.803366490103593e-06,
      "loss": 3.2031,
      "step": 48
    },
    {
      "epoch": 0.15857605177993528,
      "grad_norm": 292.0,
      "learning_rate": 4.793028253763633e-06,
      "loss": 2.1406,
      "step": 49
    },
    {
      "epoch": 0.16181229773462782,
      "grad_norm": 324.0,
      "learning_rate": 4.7824368762558595e-06,
      "loss": 2.1094,
      "step": 50
    },
    {
      "epoch": 0.1650485436893204,
      "grad_norm": 36.25,
      "learning_rate": 4.771593526826078e-06,
      "loss": 0.1162,
      "step": 51
    },
    {
      "epoch": 0.16828478964401294,
      "grad_norm": 312.0,
      "learning_rate": 4.760499402536792e-06,
      "loss": 2.8906,
      "step": 52
    },
    {
      "epoch": 0.1715210355987055,
      "grad_norm": 362.0,
      "learning_rate": 4.7491557281350455e-06,
      "loss": 3.6875,
      "step": 53
    },
    {
      "epoch": 0.17475728155339806,
      "grad_norm": 344.0,
      "learning_rate": 4.737563755917219e-06,
      "loss": 2.3594,
      "step": 54
    },
    {
      "epoch": 0.1779935275080906,
      "grad_norm": 294.0,
      "learning_rate": 4.7257247655907854e-06,
      "loss": 2.2969,
      "step": 55
    },
    {
      "epoch": 0.18122977346278318,
      "grad_norm": 310.0,
      "learning_rate": 4.7136400641330245e-06,
      "loss": 3.1875,
      "step": 56
    },
    {
      "epoch": 0.18446601941747573,
      "grad_norm": 298.0,
      "learning_rate": 4.70131098564675e-06,
      "loss": 2.3438,
      "step": 57
    },
    {
      "epoch": 0.18770226537216828,
      "grad_norm": 304.0,
      "learning_rate": 4.6887388912130206e-06,
      "loss": 2.5625,
      "step": 58
    },
    {
      "epoch": 0.19093851132686085,
      "grad_norm": 324.0,
      "learning_rate": 4.675925168740887e-06,
      "loss": 3.5312,
      "step": 59
    },
    {
      "epoch": 0.1941747572815534,
      "grad_norm": 358.0,
      "learning_rate": 4.662871232814171e-06,
      "loss": 4.0938,
      "step": 60
    },
    {
      "epoch": 0.19741100323624594,
      "grad_norm": 350.0,
      "learning_rate": 4.649578524535302e-06,
      "loss": 2.7812,
      "step": 61
    },
    {
      "epoch": 0.20064724919093851,
      "grad_norm": 346.0,
      "learning_rate": 4.636048511366222e-06,
      "loss": 2.6562,
      "step": 62
    },
    {
      "epoch": 0.20388349514563106,
      "grad_norm": 318.0,
      "learning_rate": 4.622282686966387e-06,
      "loss": 2.3906,
      "step": 63
    },
    {
      "epoch": 0.20711974110032363,
      "grad_norm": 368.0,
      "learning_rate": 4.6082825710278724e-06,
      "loss": 3.7969,
      "step": 64
    },
    {
      "epoch": 0.21035598705501618,
      "grad_norm": 326.0,
      "learning_rate": 4.594049709107604e-06,
      "loss": 2.9688,
      "step": 65
    },
    {
      "epoch": 0.21359223300970873,
      "grad_norm": 320.0,
      "learning_rate": 4.5795856724567344e-06,
      "loss": 2.3906,
      "step": 66
    },
    {
      "epoch": 0.2168284789644013,
      "grad_norm": 348.0,
      "learning_rate": 4.564892057847184e-06,
      "loss": 2.9844,
      "step": 67
    },
    {
      "epoch": 0.22006472491909385,
      "grad_norm": 324.0,
      "learning_rate": 4.549970487395365e-06,
      "loss": 2.375,
      "step": 68
    },
    {
      "epoch": 0.22330097087378642,
      "grad_norm": 324.0,
      "learning_rate": 4.534822608383104e-06,
      "loss": 2.6094,
      "step": 69
    },
    {
      "epoch": 0.22653721682847897,
      "grad_norm": 278.0,
      "learning_rate": 4.519450093075787e-06,
      "loss": 1.8125,
      "step": 70
    },
    {
      "epoch": 0.2297734627831715,
      "grad_norm": 312.0,
      "learning_rate": 4.503854638537756e-06,
      "loss": 2.9219,
      "step": 71
    },
    {
      "epoch": 0.23300970873786409,
      "grad_norm": 13.375,
      "learning_rate": 4.488037966444948e-06,
      "loss": 0.0415,
      "step": 72
    },
    {
      "epoch": 0.23624595469255663,
      "grad_norm": 316.0,
      "learning_rate": 4.472001822894839e-06,
      "loss": 2.9062,
      "step": 73
    },
    {
      "epoch": 0.23948220064724918,
      "grad_norm": 280.0,
      "learning_rate": 4.455747978213679e-06,
      "loss": 1.9453,
      "step": 74
    },
    {
      "epoch": 0.24271844660194175,
      "grad_norm": 310.0,
      "learning_rate": 4.43927822676105e-06,
      "loss": 2.7969,
      "step": 75
    },
    {
      "epoch": 0.2459546925566343,
      "grad_norm": 296.0,
      "learning_rate": 4.4225943867317835e-06,
      "loss": 2.2031,
      "step": 76
    },
    {
      "epoch": 0.24919093851132687,
      "grad_norm": 352.0,
      "learning_rate": 4.405698299955234e-06,
      "loss": 2.7344,
      "step": 77
    },
    {
      "epoch": 0.2524271844660194,
      "grad_norm": 302.0,
      "learning_rate": 4.388591831691948e-06,
      "loss": 2.4688,
      "step": 78
    },
    {
      "epoch": 0.255663430420712,
      "grad_norm": 326.0,
      "learning_rate": 4.3712768704277535e-06,
      "loss": 2.1562,
      "step": 79
    },
    {
      "epoch": 0.2588996763754045,
      "grad_norm": 302.0,
      "learning_rate": 4.353755327665268e-06,
      "loss": 2.5625,
      "step": 80
    },
    {
      "epoch": 0.2621359223300971,
      "grad_norm": 316.0,
      "learning_rate": 4.3360291377128864e-06,
      "loss": 3.2188,
      "step": 81
    },
    {
      "epoch": 0.26537216828478966,
      "grad_norm": 42.75,
      "learning_rate": 4.318100257471233e-06,
      "loss": 0.1348,
      "step": 82
    },
    {
      "epoch": 0.2686084142394822,
      "grad_norm": 308.0,
      "learning_rate": 4.299970666217135e-06,
      "loss": 2.3906,
      "step": 83
    },
    {
      "epoch": 0.27184466019417475,
      "grad_norm": 288.0,
      "learning_rate": 4.281642365385111e-06,
      "loss": 2.2031,
      "step": 84
    },
    {
      "epoch": 0.2750809061488673,
      "grad_norm": 374.0,
      "learning_rate": 4.263117378346425e-06,
      "loss": 4.0938,
      "step": 85
    },
    {
      "epoch": 0.2783171521035599,
      "grad_norm": 350.0,
      "learning_rate": 4.244397750185714e-06,
      "loss": 2.9219,
      "step": 86
    },
    {
      "epoch": 0.2815533980582524,
      "grad_norm": 318.0,
      "learning_rate": 4.225485547475217e-06,
      "loss": 3.5781,
      "step": 87
    },
    {
      "epoch": 0.284789644012945,
      "grad_norm": 370.0,
      "learning_rate": 4.206382858046636e-06,
      "loss": 4.0938,
      "step": 88
    },
    {
      "epoch": 0.28802588996763756,
      "grad_norm": 282.0,
      "learning_rate": 4.187091790760644e-06,
      "loss": 1.8359,
      "step": 89
    },
    {
      "epoch": 0.2912621359223301,
      "grad_norm": 302.0,
      "learning_rate": 4.167614475274082e-06,
      "loss": 2.2812,
      "step": 90
    },
    {
      "epoch": 0.29449838187702265,
      "grad_norm": 292.0,
      "learning_rate": 4.147953061804845e-06,
      "loss": 2.2188,
      "step": 91
    },
    {
      "epoch": 0.2977346278317152,
      "grad_norm": 298.0,
      "learning_rate": 4.128109720894512e-06,
      "loss": 2.5,
      "step": 92
    },
    {
      "epoch": 0.30097087378640774,
      "grad_norm": 346.0,
      "learning_rate": 4.108086643168724e-06,
      "loss": 2.9375,
      "step": 93
    },
    {
      "epoch": 0.3042071197411003,
      "grad_norm": 352.0,
      "learning_rate": 4.087886039095353e-06,
      "loss": 3.2812,
      "step": 94
    },
    {
      "epoch": 0.3074433656957929,
      "grad_norm": 294.0,
      "learning_rate": 4.067510138740467e-06,
      "loss": 2.3125,
      "step": 95
    },
    {
      "epoch": 0.3106796116504854,
      "grad_norm": 298.0,
      "learning_rate": 4.046961191522147e-06,
      "loss": 2.2969,
      "step": 96
    },
    {
      "epoch": 0.313915857605178,
      "grad_norm": 310.0,
      "learning_rate": 4.026241465962154e-06,
      "loss": 2.6094,
      "step": 97
    },
    {
      "epoch": 0.31715210355987056,
      "grad_norm": 320.0,
      "learning_rate": 4.0053532494354985e-06,
      "loss": 2.3281,
      "step": 98
    },
    {
      "epoch": 0.32038834951456313,
      "grad_norm": 35.25,
      "learning_rate": 3.984298847917923e-06,
      "loss": 0.1133,
      "step": 99
    },
    {
      "epoch": 0.32362459546925565,
      "grad_norm": 346.0,
      "learning_rate": 3.963080585731324e-06,
      "loss": 3.4688,
      "step": 100
    },
    {
      "epoch": 0.3268608414239482,
      "grad_norm": 364.0,
      "learning_rate": 3.941700805287169e-06,
      "loss": 3.2969,
      "step": 101
    },
    {
      "epoch": 0.3300970873786408,
      "grad_norm": 358.0,
      "learning_rate": 3.92016186682789e-06,
      "loss": 3.7656,
      "step": 102
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 294.0,
      "learning_rate": 3.898466148166333e-06,
      "loss": 1.8672,
      "step": 103
    },
    {
      "epoch": 0.3365695792880259,
      "grad_norm": 314.0,
      "learning_rate": 3.876616044423253e-06,
      "loss": 2.4531,
      "step": 104
    },
    {
      "epoch": 0.33980582524271846,
      "grad_norm": 250.0,
      "learning_rate": 3.854613967762898e-06,
      "loss": 1.3906,
      "step": 105
    },
    {
      "epoch": 0.343042071197411,
      "grad_norm": 314.0,
      "learning_rate": 3.832462347126722e-06,
      "loss": 2.875,
      "step": 106
    },
    {
      "epoch": 0.34627831715210355,
      "grad_norm": 326.0,
      "learning_rate": 3.8101636279652375e-06,
      "loss": 3.7656,
      "step": 107
    },
    {
      "epoch": 0.34951456310679613,
      "grad_norm": 306.0,
      "learning_rate": 3.787720271968046e-06,
      "loss": 2.625,
      "step": 108
    },
    {
      "epoch": 0.35275080906148865,
      "grad_norm": 300.0,
      "learning_rate": 3.765134756792079e-06,
      "loss": 2.1875,
      "step": 109
    },
    {
      "epoch": 0.3559870550161812,
      "grad_norm": 318.0,
      "learning_rate": 3.742409575788074e-06,
      "loss": 3.1406,
      "step": 110
    },
    {
      "epoch": 0.3592233009708738,
      "grad_norm": 332.0,
      "learning_rate": 3.719547237725319e-06,
      "loss": 2.5469,
      "step": 111
    },
    {
      "epoch": 0.36245954692556637,
      "grad_norm": 260.0,
      "learning_rate": 3.6965502665146916e-06,
      "loss": 1.5312,
      "step": 112
    },
    {
      "epoch": 0.3656957928802589,
      "grad_norm": 308.0,
      "learning_rate": 3.6734212009300346e-06,
      "loss": 1.9688,
      "step": 113
    },
    {
      "epoch": 0.36893203883495146,
      "grad_norm": 374.0,
      "learning_rate": 3.650162594327881e-06,
      "loss": 4.3438,
      "step": 114
    },
    {
      "epoch": 0.37216828478964403,
      "grad_norm": 310.0,
      "learning_rate": 3.6267770143655743e-06,
      "loss": 2.9688,
      "step": 115
    },
    {
      "epoch": 0.37540453074433655,
      "grad_norm": 298.0,
      "learning_rate": 3.603267042717813e-06,
      "loss": 1.9297,
      "step": 116
    },
    {
      "epoch": 0.3786407766990291,
      "grad_norm": 342.0,
      "learning_rate": 3.579635274791639e-06,
      "loss": 2.4531,
      "step": 117
    },
    {
      "epoch": 0.3818770226537217,
      "grad_norm": 304.0,
      "learning_rate": 3.555884319439917e-06,
      "loss": 2.5781,
      "step": 118
    },
    {
      "epoch": 0.3851132686084142,
      "grad_norm": 290.0,
      "learning_rate": 3.532016798673329e-06,
      "loss": 2.0,
      "step": 119
    },
    {
      "epoch": 0.3883495145631068,
      "grad_norm": 306.0,
      "learning_rate": 3.508035347370912e-06,
      "loss": 2.6094,
      "step": 120
    },
    {
      "epoch": 0.39158576051779936,
      "grad_norm": 358.0,
      "learning_rate": 3.483942612989183e-06,
      "loss": 3.4062,
      "step": 121
    },
    {
      "epoch": 0.3948220064724919,
      "grad_norm": 364.0,
      "learning_rate": 3.4597412552698617e-06,
      "loss": 3.4375,
      "step": 122
    },
    {
      "epoch": 0.39805825242718446,
      "grad_norm": 322.0,
      "learning_rate": 3.4354339459462556e-06,
      "loss": 3.5469,
      "step": 123
    },
    {
      "epoch": 0.40129449838187703,
      "grad_norm": 372.0,
      "learning_rate": 3.4110233684483033e-06,
      "loss": 4.375,
      "step": 124
    },
    {
      "epoch": 0.4045307443365696,
      "grad_norm": 304.0,
      "learning_rate": 3.386512217606339e-06,
      "loss": 2.4688,
      "step": 125
    },
    {
      "epoch": 0.4077669902912621,
      "grad_norm": 304.0,
      "learning_rate": 3.361903199353593e-06,
      "loss": 2.5781,
      "step": 126
    },
    {
      "epoch": 0.4110032362459547,
      "grad_norm": 302.0,
      "learning_rate": 3.3371990304274654e-06,
      "loss": 2.2969,
      "step": 127
    },
    {
      "epoch": 0.41423948220064727,
      "grad_norm": 308.0,
      "learning_rate": 3.3124024380696134e-06,
      "loss": 2.8281,
      "step": 128
    },
    {
      "epoch": 0.4174757281553398,
      "grad_norm": 356.0,
      "learning_rate": 3.28751615972487e-06,
      "loss": 2.7812,
      "step": 129
    },
    {
      "epoch": 0.42071197411003236,
      "grad_norm": 314.0,
      "learning_rate": 3.262542942739044e-06,
      "loss": 2.9375,
      "step": 130
    },
    {
      "epoch": 0.42394822006472493,
      "grad_norm": 276.0,
      "learning_rate": 3.2374855440556242e-06,
      "loss": 1.6406,
      "step": 131
    },
    {
      "epoch": 0.42718446601941745,
      "grad_norm": 306.0,
      "learning_rate": 3.2123467299114216e-06,
      "loss": 2.7188,
      "step": 132
    },
    {
      "epoch": 0.43042071197411,
      "grad_norm": 312.0,
      "learning_rate": 3.1871292755311887e-06,
      "loss": 3.0469,
      "step": 133
    },
    {
      "epoch": 0.4336569579288026,
      "grad_norm": 322.0,
      "learning_rate": 3.1618359648212492e-06,
      "loss": 3.0156,
      "step": 134
    },
    {
      "epoch": 0.4368932038834951,
      "grad_norm": 330.0,
      "learning_rate": 3.136469590062158e-06,
      "loss": 2.5469,
      "step": 135
    },
    {
      "epoch": 0.4401294498381877,
      "grad_norm": 302.0,
      "learning_rate": 3.1110329516004546e-06,
      "loss": 1.8828,
      "step": 136
    },
    {
      "epoch": 0.44336569579288027,
      "grad_norm": 314.0,
      "learning_rate": 3.085528857539506e-06,
      "loss": 2.7812,
      "step": 137
    },
    {
      "epoch": 0.44660194174757284,
      "grad_norm": 272.0,
      "learning_rate": 3.0599601234295124e-06,
      "loss": 1.7734,
      "step": 138
    },
    {
      "epoch": 0.44983818770226536,
      "grad_norm": 338.0,
      "learning_rate": 3.0343295719566747e-06,
      "loss": 2.9688,
      "step": 139
    },
    {
      "epoch": 0.45307443365695793,
      "grad_norm": 334.0,
      "learning_rate": 3.0086400326315853e-06,
      "loss": 2.7656,
      "step": 140
    },
    {
      "epoch": 0.4563106796116505,
      "grad_norm": 300.0,
      "learning_rate": 2.9828943414768583e-06,
      "loss": 2.3594,
      "step": 141
    },
    {
      "epoch": 0.459546925566343,
      "grad_norm": 328.0,
      "learning_rate": 2.957095340714049e-06,
      "loss": 2.0781,
      "step": 142
    },
    {
      "epoch": 0.4627831715210356,
      "grad_norm": 306.0,
      "learning_rate": 2.9312458784498763e-06,
      "loss": 2.5938,
      "step": 143
    },
    {
      "epoch": 0.46601941747572817,
      "grad_norm": 334.0,
      "learning_rate": 2.9053488083618118e-06,
      "loss": 2.375,
      "step": 144
    },
    {
      "epoch": 0.4692556634304207,
      "grad_norm": 276.0,
      "learning_rate": 2.8794069893830386e-06,
      "loss": 1.8281,
      "step": 145
    },
    {
      "epoch": 0.47249190938511326,
      "grad_norm": 340.0,
      "learning_rate": 2.8534232853868384e-06,
      "loss": 2.6406,
      "step": 146
    },
    {
      "epoch": 0.47572815533980584,
      "grad_norm": 282.0,
      "learning_rate": 2.8274005648704316e-06,
      "loss": 1.9062,
      "step": 147
    },
    {
      "epoch": 0.47896440129449835,
      "grad_norm": 253.0,
      "learning_rate": 2.8013417006383078e-06,
      "loss": 1.4141,
      "step": 148
    },
    {
      "epoch": 0.48220064724919093,
      "grad_norm": 334.0,
      "learning_rate": 2.775249569485079e-06,
      "loss": 2.125,
      "step": 149
    },
    {
      "epoch": 0.4854368932038835,
      "grad_norm": 133.0,
      "learning_rate": 2.7491270518778913e-06,
      "loss": 0.4941,
      "step": 150
    },
    {
      "epoch": 0.4886731391585761,
      "grad_norm": 316.0,
      "learning_rate": 2.722977031638435e-06,
      "loss": 2.5469,
      "step": 151
    },
    {
      "epoch": 0.4919093851132686,
      "grad_norm": 296.0,
      "learning_rate": 2.696802395624579e-06,
      "loss": 2.0625,
      "step": 152
    },
    {
      "epoch": 0.49514563106796117,
      "grad_norm": 302.0,
      "learning_rate": 2.670606033411678e-06,
      "loss": 2.625,
      "step": 153
    },
    {
      "epoch": 0.49838187702265374,
      "grad_norm": 47.25,
      "learning_rate": 2.6443908369735715e-06,
      "loss": 0.1514,
      "step": 154
    },
    {
      "epoch": 0.5016181229773463,
      "grad_norm": 344.0,
      "learning_rate": 2.6181597003633218e-06,
      "loss": 3.2188,
      "step": 155
    },
    {
      "epoch": 0.5048543689320388,
      "grad_norm": 316.0,
      "learning_rate": 2.5919155193937244e-06,
      "loss": 3.0469,
      "step": 156
    },
    {
      "epoch": 0.5080906148867314,
      "grad_norm": 322.0,
      "learning_rate": 2.565661191317618e-06,
      "loss": 2.625,
      "step": 157
    },
    {
      "epoch": 0.511326860841424,
      "grad_norm": 126.0,
      "learning_rate": 2.5393996145080413e-06,
      "loss": 0.4668,
      "step": 158
    },
    {
      "epoch": 0.5145631067961165,
      "grad_norm": 344.0,
      "learning_rate": 2.5131336881382658e-06,
      "loss": 2.75,
      "step": 159
    },
    {
      "epoch": 0.517799352750809,
      "grad_norm": 318.0,
      "learning_rate": 2.4868663118617355e-06,
      "loss": 1.9453,
      "step": 160
    },
    {
      "epoch": 0.5210355987055016,
      "grad_norm": 370.0,
      "learning_rate": 2.4606003854919595e-06,
      "loss": 3.8594,
      "step": 161
    },
    {
      "epoch": 0.5242718446601942,
      "grad_norm": 316.0,
      "learning_rate": 2.4343388086823828e-06,
      "loss": 3.125,
      "step": 162
    },
    {
      "epoch": 0.5275080906148867,
      "grad_norm": 320.0,
      "learning_rate": 2.4080844806062764e-06,
      "loss": 3.2031,
      "step": 163
    },
    {
      "epoch": 0.5307443365695793,
      "grad_norm": 324.0,
      "learning_rate": 2.3818402996366786e-06,
      "loss": 3.8125,
      "step": 164
    },
    {
      "epoch": 0.5339805825242718,
      "grad_norm": 324.0,
      "learning_rate": 2.3556091630264294e-06,
      "loss": 2.75,
      "step": 165
    },
    {
      "epoch": 0.5372168284789643,
      "grad_norm": 336.0,
      "learning_rate": 2.3293939665883233e-06,
      "loss": 2.8438,
      "step": 166
    },
    {
      "epoch": 0.540453074433657,
      "grad_norm": 248.0,
      "learning_rate": 2.303197604375422e-06,
      "loss": 1.1953,
      "step": 167
    },
    {
      "epoch": 0.5436893203883495,
      "grad_norm": 312.0,
      "learning_rate": 2.277022968361566e-06,
      "loss": 2.7812,
      "step": 168
    },
    {
      "epoch": 0.5469255663430421,
      "grad_norm": 300.0,
      "learning_rate": 2.2508729481221096e-06,
      "loss": 1.6406,
      "step": 169
    },
    {
      "epoch": 0.5501618122977346,
      "grad_norm": 294.0,
      "learning_rate": 2.2247504305149217e-06,
      "loss": 2.25,
      "step": 170
    },
    {
      "epoch": 0.5533980582524272,
      "grad_norm": 340.0,
      "learning_rate": 2.1986582993616926e-06,
      "loss": 2.625,
      "step": 171
    },
    {
      "epoch": 0.5566343042071198,
      "grad_norm": 318.0,
      "learning_rate": 2.1725994351295697e-06,
      "loss": 2.6406,
      "step": 172
    },
    {
      "epoch": 0.5598705501618123,
      "grad_norm": 332.0,
      "learning_rate": 2.1465767146131633e-06,
      "loss": 2.5625,
      "step": 173
    },
    {
      "epoch": 0.5631067961165048,
      "grad_norm": 282.0,
      "learning_rate": 2.1205930106169626e-06,
      "loss": 1.8984,
      "step": 174
    },
    {
      "epoch": 0.5663430420711975,
      "grad_norm": 288.0,
      "learning_rate": 2.094651191638189e-06,
      "loss": 2.0312,
      "step": 175
    },
    {
      "epoch": 0.56957928802589,
      "grad_norm": 342.0,
      "learning_rate": 2.0687541215501245e-06,
      "loss": 2.5156,
      "step": 176
    },
    {
      "epoch": 0.5728155339805825,
      "grad_norm": 382.0,
      "learning_rate": 2.0429046592859524e-06,
      "loss": 4.0,
      "step": 177
    },
    {
      "epoch": 0.5760517799352751,
      "grad_norm": 318.0,
      "learning_rate": 2.0171056585231425e-06,
      "loss": 3.1562,
      "step": 178
    },
    {
      "epoch": 0.5792880258899676,
      "grad_norm": 266.0,
      "learning_rate": 1.991359967368416e-06,
      "loss": 1.2578,
      "step": 179
    },
    {
      "epoch": 0.5825242718446602,
      "grad_norm": 294.0,
      "learning_rate": 1.965670428043326e-06,
      "loss": 2.1719,
      "step": 180
    },
    {
      "epoch": 0.5857605177993528,
      "grad_norm": 336.0,
      "learning_rate": 1.940039876570489e-06,
      "loss": 2.7812,
      "step": 181
    },
    {
      "epoch": 0.5889967637540453,
      "grad_norm": 340.0,
      "learning_rate": 1.914471142460495e-06,
      "loss": 3.3125,
      "step": 182
    },
    {
      "epoch": 0.5922330097087378,
      "grad_norm": 336.0,
      "learning_rate": 1.888967048399547e-06,
      "loss": 2.3906,
      "step": 183
    },
    {
      "epoch": 0.5954692556634305,
      "grad_norm": 318.0,
      "learning_rate": 1.8635304099378426e-06,
      "loss": 2.9531,
      "step": 184
    },
    {
      "epoch": 0.598705501618123,
      "grad_norm": 43.5,
      "learning_rate": 1.8381640351787516e-06,
      "loss": 0.1416,
      "step": 185
    },
    {
      "epoch": 0.6019417475728155,
      "grad_norm": 326.0,
      "learning_rate": 1.8128707244688109e-06,
      "loss": 2.6875,
      "step": 186
    },
    {
      "epoch": 0.6051779935275081,
      "grad_norm": 314.0,
      "learning_rate": 1.7876532700885788e-06,
      "loss": 2.1094,
      "step": 187
    },
    {
      "epoch": 0.6084142394822006,
      "grad_norm": 334.0,
      "learning_rate": 1.7625144559443758e-06,
      "loss": 2.8594,
      "step": 188
    },
    {
      "epoch": 0.6116504854368932,
      "grad_norm": 360.0,
      "learning_rate": 1.7374570572609559e-06,
      "loss": 3.2188,
      "step": 189
    },
    {
      "epoch": 0.6148867313915858,
      "grad_norm": 362.0,
      "learning_rate": 1.7124838402751304e-06,
      "loss": 3.5156,
      "step": 190
    },
    {
      "epoch": 0.6181229773462783,
      "grad_norm": 290.0,
      "learning_rate": 1.6875975619303872e-06,
      "loss": 2.1562,
      "step": 191
    },
    {
      "epoch": 0.6213592233009708,
      "grad_norm": 358.0,
      "learning_rate": 1.6628009695725348e-06,
      "loss": 4.8438,
      "step": 192
    },
    {
      "epoch": 0.6245954692556634,
      "grad_norm": 328.0,
      "learning_rate": 1.6380968006464073e-06,
      "loss": 2.7969,
      "step": 193
    },
    {
      "epoch": 0.627831715210356,
      "grad_norm": 332.0,
      "learning_rate": 1.613487782393661e-06,
      "loss": 2.4219,
      "step": 194
    },
    {
      "epoch": 0.6310679611650486,
      "grad_norm": 332.0,
      "learning_rate": 1.588976631551697e-06,
      "loss": 2.6562,
      "step": 195
    },
    {
      "epoch": 0.6343042071197411,
      "grad_norm": 312.0,
      "learning_rate": 1.5645660540537444e-06,
      "loss": 3.0469,
      "step": 196
    },
    {
      "epoch": 0.6375404530744336,
      "grad_norm": 282.0,
      "learning_rate": 1.5402587447301387e-06,
      "loss": 1.9531,
      "step": 197
    },
    {
      "epoch": 0.6407766990291263,
      "grad_norm": 346.0,
      "learning_rate": 1.516057387010818e-06,
      "loss": 3.5938,
      "step": 198
    },
    {
      "epoch": 0.6440129449838188,
      "grad_norm": 362.0,
      "learning_rate": 1.4919646526290884e-06,
      "loss": 3.3906,
      "step": 199
    },
    {
      "epoch": 0.6472491909385113,
      "grad_norm": 306.0,
      "learning_rate": 1.4679832013266721e-06,
      "loss": 2.5,
      "step": 200
    },
    {
      "epoch": 0.6504854368932039,
      "grad_norm": 228.0,
      "learning_rate": 1.4441156805600842e-06,
      "loss": 1.0859,
      "step": 201
    },
    {
      "epoch": 0.6537216828478964,
      "grad_norm": 362.0,
      "learning_rate": 1.4203647252083619e-06,
      "loss": 3.125,
      "step": 202
    },
    {
      "epoch": 0.656957928802589,
      "grad_norm": 310.0,
      "learning_rate": 1.3967329572821875e-06,
      "loss": 2.9062,
      "step": 203
    },
    {
      "epoch": 0.6601941747572816,
      "grad_norm": 306.0,
      "learning_rate": 1.3732229856344259e-06,
      "loss": 2.3438,
      "step": 204
    },
    {
      "epoch": 0.6634304207119741,
      "grad_norm": 270.0,
      "learning_rate": 1.3498374056721198e-06,
      "loss": 1.7188,
      "step": 205
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 284.0,
      "learning_rate": 1.326578799069966e-06,
      "loss": 1.8203,
      "step": 206
    },
    {
      "epoch": 0.6699029126213593,
      "grad_norm": 167.0,
      "learning_rate": 1.3034497334853092e-06,
      "loss": 0.6914,
      "step": 207
    },
    {
      "epoch": 0.6731391585760518,
      "grad_norm": 32.5,
      "learning_rate": 1.280452762274682e-06,
      "loss": 0.1016,
      "step": 208
    },
    {
      "epoch": 0.6763754045307443,
      "grad_norm": 326.0,
      "learning_rate": 1.2575904242119264e-06,
      "loss": 2.3125,
      "step": 209
    },
    {
      "epoch": 0.6796116504854369,
      "grad_norm": 308.0,
      "learning_rate": 1.234865243207921e-06,
      "loss": 2.7969,
      "step": 210
    },
    {
      "epoch": 0.6828478964401294,
      "grad_norm": 25.625,
      "learning_rate": 1.2122797280319543e-06,
      "loss": 0.0806,
      "step": 211
    },
    {
      "epoch": 0.686084142394822,
      "grad_norm": 320.0,
      "learning_rate": 1.1898363720347635e-06,
      "loss": 2.6719,
      "step": 212
    },
    {
      "epoch": 0.6893203883495146,
      "grad_norm": 308.0,
      "learning_rate": 1.167537652873279e-06,
      "loss": 2.7656,
      "step": 213
    },
    {
      "epoch": 0.6925566343042071,
      "grad_norm": 38.25,
      "learning_rate": 1.1453860322371032e-06,
      "loss": 0.1162,
      "step": 214
    },
    {
      "epoch": 0.6957928802588996,
      "grad_norm": 274.0,
      "learning_rate": 1.1233839555767482e-06,
      "loss": 1.75,
      "step": 215
    },
    {
      "epoch": 0.6990291262135923,
      "grad_norm": 296.0,
      "learning_rate": 1.1015338518336672e-06,
      "loss": 2.2656,
      "step": 216
    },
    {
      "epoch": 0.7022653721682848,
      "grad_norm": 21.5,
      "learning_rate": 1.079838133172111e-06,
      "loss": 0.0674,
      "step": 217
    },
    {
      "epoch": 0.7055016181229773,
      "grad_norm": 19.875,
      "learning_rate": 1.0582991947128324e-06,
      "loss": 0.0613,
      "step": 218
    },
    {
      "epoch": 0.7087378640776699,
      "grad_norm": 322.0,
      "learning_rate": 1.0369194142686766e-06,
      "loss": 3.0625,
      "step": 219
    },
    {
      "epoch": 0.7119741100323624,
      "grad_norm": 165.0,
      "learning_rate": 1.0157011520820784e-06,
      "loss": 0.6406,
      "step": 220
    },
    {
      "epoch": 0.7152103559870551,
      "grad_norm": 354.0,
      "learning_rate": 9.946467505645019e-07,
      "loss": 2.875,
      "step": 221
    },
    {
      "epoch": 0.7184466019417476,
      "grad_norm": 354.0,
      "learning_rate": 9.73758534037847e-07,
      "loss": 3.8594,
      "step": 222
    },
    {
      "epoch": 0.7216828478964401,
      "grad_norm": 282.0,
      "learning_rate": 9.530388084778541e-07,
      "loss": 1.8594,
      "step": 223
    },
    {
      "epoch": 0.7249190938511327,
      "grad_norm": 264.0,
      "learning_rate": 9.32489861259534e-07,
      "loss": 1.5312,
      "step": 224
    },
    {
      "epoch": 0.7281553398058253,
      "grad_norm": 276.0,
      "learning_rate": 9.121139609046484e-07,
      "loss": 1.8047,
      "step": 225
    },
    {
      "epoch": 0.7313915857605178,
      "grad_norm": 298.0,
      "learning_rate": 8.919133568312768e-07,
      "loss": 2.3438,
      "step": 226
    },
    {
      "epoch": 0.7346278317152104,
      "grad_norm": 290.0,
      "learning_rate": 8.718902791054895e-07,
      "loss": 2.1406,
      "step": 227
    },
    {
      "epoch": 0.7378640776699029,
      "grad_norm": 312.0,
      "learning_rate": 8.52046938195156e-07,
      "loss": 2.8594,
      "step": 228
    },
    {
      "epoch": 0.7411003236245954,
      "grad_norm": 278.0,
      "learning_rate": 8.323855247259185e-07,
      "loss": 1.5469,
      "step": 229
    },
    {
      "epoch": 0.7443365695792881,
      "grad_norm": 312.0,
      "learning_rate": 8.129082092393562e-07,
      "loss": 2.3906,
      "step": 230
    },
    {
      "epoch": 0.7475728155339806,
      "grad_norm": 338.0,
      "learning_rate": 7.936171419533653e-07,
      "loss": 3.4062,
      "step": 231
    },
    {
      "epoch": 0.7508090614886731,
      "grad_norm": 354.0,
      "learning_rate": 7.745144525247839e-07,
      "loss": 3.7344,
      "step": 232
    },
    {
      "epoch": 0.7540453074433657,
      "grad_norm": 288.0,
      "learning_rate": 7.55602249814287e-07,
      "loss": 2.1719,
      "step": 233
    },
    {
      "epoch": 0.7572815533980582,
      "grad_norm": 286.0,
      "learning_rate": 7.368826216535758e-07,
      "loss": 1.9062,
      "step": 234
    },
    {
      "epoch": 0.7605177993527508,
      "grad_norm": 280.0,
      "learning_rate": 7.183576346148899e-07,
      "loss": 1.8672,
      "step": 235
    },
    {
      "epoch": 0.7637540453074434,
      "grad_norm": 258.0,
      "learning_rate": 7.000293337828656e-07,
      "loss": 1.4375,
      "step": 236
    },
    {
      "epoch": 0.7669902912621359,
      "grad_norm": 312.0,
      "learning_rate": 6.818997425287671e-07,
      "loss": 2.7188,
      "step": 237
    },
    {
      "epoch": 0.7702265372168284,
      "grad_norm": 370.0,
      "learning_rate": 6.639708622871144e-07,
      "loss": 3.5312,
      "step": 238
    },
    {
      "epoch": 0.7734627831715211,
      "grad_norm": 278.0,
      "learning_rate": 6.462446723347324e-07,
      "loss": 1.8828,
      "step": 239
    },
    {
      "epoch": 0.7766990291262136,
      "grad_norm": 320.0,
      "learning_rate": 6.28723129572247e-07,
      "loss": 3.2656,
      "step": 240
    },
    {
      "epoch": 0.7799352750809061,
      "grad_norm": 296.0,
      "learning_rate": 6.11408168308052e-07,
      "loss": 2.0,
      "step": 241
    },
    {
      "epoch": 0.7831715210355987,
      "grad_norm": 346.0,
      "learning_rate": 5.943017000447671e-07,
      "loss": 3.7344,
      "step": 242
    },
    {
      "epoch": 0.7864077669902912,
      "grad_norm": 292.0,
      "learning_rate": 5.774056132682168e-07,
      "loss": 2.3438,
      "step": 243
    },
    {
      "epoch": 0.7896440129449838,
      "grad_norm": 268.0,
      "learning_rate": 5.607217732389503e-07,
      "loss": 1.6719,
      "step": 244
    },
    {
      "epoch": 0.7928802588996764,
      "grad_norm": 292.0,
      "learning_rate": 5.442520217863215e-07,
      "loss": 2.1875,
      "step": 245
    },
    {
      "epoch": 0.7961165048543689,
      "grad_norm": 120.0,
      "learning_rate": 5.279981771051615e-07,
      "loss": 0.3965,
      "step": 246
    },
    {
      "epoch": 0.7993527508090615,
      "grad_norm": 346.0,
      "learning_rate": 5.119620335550532e-07,
      "loss": 3.0938,
      "step": 247
    },
    {
      "epoch": 0.8025889967637541,
      "grad_norm": 282.0,
      "learning_rate": 4.961453614622453e-07,
      "loss": 1.5625,
      "step": 248
    },
    {
      "epoch": 0.8058252427184466,
      "grad_norm": 286.0,
      "learning_rate": 4.805499069242131e-07,
      "loss": 1.9531,
      "step": 249
    },
    {
      "epoch": 0.8090614886731392,
      "grad_norm": 328.0,
      "learning_rate": 4.651773916168967e-07,
      "loss": 2.1875,
      "step": 250
    },
    {
      "epoch": 0.8122977346278317,
      "grad_norm": 306.0,
      "learning_rate": 4.5002951260463503e-07,
      "loss": 2.375,
      "step": 251
    },
    {
      "epoch": 0.8155339805825242,
      "grad_norm": 348.0,
      "learning_rate": 4.3510794215281595e-07,
      "loss": 3.0156,
      "step": 252
    },
    {
      "epoch": 0.8187702265372169,
      "grad_norm": 304.0,
      "learning_rate": 4.2041432754326593e-07,
      "loss": 2.5938,
      "step": 253
    },
    {
      "epoch": 0.8220064724919094,
      "grad_norm": 326.0,
      "learning_rate": 4.059502908923962e-07,
      "loss": 2.2969,
      "step": 254
    },
    {
      "epoch": 0.8252427184466019,
      "grad_norm": 290.0,
      "learning_rate": 3.917174289721276e-07,
      "loss": 2.1562,
      "step": 255
    },
    {
      "epoch": 0.8284789644012945,
      "grad_norm": 316.0,
      "learning_rate": 3.7771731303361314e-07,
      "loss": 3.2344,
      "step": 256
    },
    {
      "epoch": 0.8317152103559871,
      "grad_norm": 314.0,
      "learning_rate": 3.639514886337786e-07,
      "loss": 2.5469,
      "step": 257
    },
    {
      "epoch": 0.8349514563106796,
      "grad_norm": 251.0,
      "learning_rate": 3.5042147546469894e-07,
      "loss": 1.3984,
      "step": 258
    },
    {
      "epoch": 0.8381877022653722,
      "grad_norm": 312.0,
      "learning_rate": 3.371287671858292e-07,
      "loss": 2.7812,
      "step": 259
    },
    {
      "epoch": 0.8414239482200647,
      "grad_norm": 318.0,
      "learning_rate": 3.2407483125911354e-07,
      "loss": 2.5469,
      "step": 260
    },
    {
      "epoch": 0.8446601941747572,
      "grad_norm": 308.0,
      "learning_rate": 3.112611087869799e-07,
      "loss": 2.7188,
      "step": 261
    },
    {
      "epoch": 0.8478964401294499,
      "grad_norm": 284.0,
      "learning_rate": 2.9868901435325033e-07,
      "loss": 2.0469,
      "step": 262
    },
    {
      "epoch": 0.8511326860841424,
      "grad_norm": 322.0,
      "learning_rate": 2.8635993586697555e-07,
      "loss": 2.6562,
      "step": 263
    },
    {
      "epoch": 0.8543689320388349,
      "grad_norm": 340.0,
      "learning_rate": 2.7427523440921534e-07,
      "loss": 2.8438,
      "step": 264
    },
    {
      "epoch": 0.8576051779935275,
      "grad_norm": 298.0,
      "learning_rate": 2.62436244082781e-07,
      "loss": 2.4688,
      "step": 265
    },
    {
      "epoch": 0.86084142394822,
      "grad_norm": 288.0,
      "learning_rate": 2.5084427186495566e-07,
      "loss": 1.9375,
      "step": 266
    },
    {
      "epoch": 0.8640776699029126,
      "grad_norm": 282.0,
      "learning_rate": 2.3950059746320864e-07,
      "loss": 1.6953,
      "step": 267
    },
    {
      "epoch": 0.8673139158576052,
      "grad_norm": 320.0,
      "learning_rate": 2.2840647317392218e-07,
      "loss": 2.6719,
      "step": 268
    },
    {
      "epoch": 0.8705501618122977,
      "grad_norm": 286.0,
      "learning_rate": 2.1756312374414113e-07,
      "loss": 2.125,
      "step": 269
    },
    {
      "epoch": 0.8737864077669902,
      "grad_norm": 318.0,
      "learning_rate": 2.0697174623636795e-07,
      "loss": 3.3125,
      "step": 270
    },
    {
      "epoch": 0.8770226537216829,
      "grad_norm": 314.0,
      "learning_rate": 1.9663350989640812e-07,
      "loss": 3.1562,
      "step": 271
    },
    {
      "epoch": 0.8802588996763754,
      "grad_norm": 306.0,
      "learning_rate": 1.8654955602429499e-07,
      "loss": 2.7031,
      "step": 272
    },
    {
      "epoch": 0.883495145631068,
      "grad_norm": 344.0,
      "learning_rate": 1.7672099784829116e-07,
      "loss": 3.0156,
      "step": 273
    },
    {
      "epoch": 0.8867313915857605,
      "grad_norm": 332.0,
      "learning_rate": 1.6714892040199383e-07,
      "loss": 2.7031,
      "step": 274
    },
    {
      "epoch": 0.889967637540453,
      "grad_norm": 15.25,
      "learning_rate": 1.5783438040455097e-07,
      "loss": 0.0471,
      "step": 275
    },
    {
      "epoch": 0.8932038834951457,
      "grad_norm": 316.0,
      "learning_rate": 1.4877840614400452e-07,
      "loss": 2.1094,
      "step": 276
    },
    {
      "epoch": 0.8964401294498382,
      "grad_norm": 318.0,
      "learning_rate": 1.399819973637706e-07,
      "loss": 2.7188,
      "step": 277
    },
    {
      "epoch": 0.8996763754045307,
      "grad_norm": 352.0,
      "learning_rate": 1.3144612515227278e-07,
      "loss": 3.3438,
      "step": 278
    },
    {
      "epoch": 0.9029126213592233,
      "grad_norm": 366.0,
      "learning_rate": 1.2317173183573616e-07,
      "loss": 3.375,
      "step": 279
    },
    {
      "epoch": 0.9061488673139159,
      "grad_norm": 288.0,
      "learning_rate": 1.151597308741606e-07,
      "loss": 1.9375,
      "step": 280
    },
    {
      "epoch": 0.9093851132686084,
      "grad_norm": 302.0,
      "learning_rate": 1.0741100676047639e-07,
      "loss": 1.7656,
      "step": 281
    },
    {
      "epoch": 0.912621359223301,
      "grad_norm": 308.0,
      "learning_rate": 9.992641492290094e-08,
      "loss": 2.0469,
      "step": 282
    },
    {
      "epoch": 0.9158576051779935,
      "grad_norm": 298.0,
      "learning_rate": 9.270678163050218e-08,
      "loss": 2.2188,
      "step": 283
    },
    {
      "epoch": 0.919093851132686,
      "grad_norm": 286.0,
      "learning_rate": 8.575290390198193e-08,
      "loss": 2.0,
      "step": 284
    },
    {
      "epoch": 0.9223300970873787,
      "grad_norm": 360.0,
      "learning_rate": 7.906554941768896e-08,
      "loss": 3.8438,
      "step": 285
    },
    {
      "epoch": 0.9255663430420712,
      "grad_norm": 342.0,
      "learning_rate": 7.264545643486997e-08,
      "loss": 3.0312,
      "step": 286
    },
    {
      "epoch": 0.9288025889967637,
      "grad_norm": 310.0,
      "learning_rate": 6.649333370616712e-08,
      "loss": 2.7031,
      "step": 287
    },
    {
      "epoch": 0.9320388349514563,
      "grad_norm": 254.0,
      "learning_rate": 6.060986040137689e-08,
      "loss": 1.4219,
      "step": 288
    },
    {
      "epoch": 0.9352750809061489,
      "grad_norm": 312.0,
      "learning_rate": 5.4995686032471575e-08,
      "loss": 2.8125,
      "step": 289
    },
    {
      "epoch": 0.9385113268608414,
      "grad_norm": 312.0,
      "learning_rate": 4.96514303818954e-08,
      "loss": 2.7188,
      "step": 290
    },
    {
      "epoch": 0.941747572815534,
      "grad_norm": 308.0,
      "learning_rate": 4.457768343414382e-08,
      "loss": 2.0312,
      "step": 291
    },
    {
      "epoch": 0.9449838187702265,
      "grad_norm": 338.0,
      "learning_rate": 3.9775005310629946e-08,
      "loss": 2.8125,
      "step": 292
    },
    {
      "epoch": 0.948220064724919,
      "grad_norm": 330.0,
      "learning_rate": 3.5243926207851606e-08,
      "loss": 3.0156,
      "step": 293
    },
    {
      "epoch": 0.9514563106796117,
      "grad_norm": 284.0,
      "learning_rate": 3.09849463388584e-08,
      "loss": 1.8516,
      "step": 294
    },
    {
      "epoch": 0.9546925566343042,
      "grad_norm": 352.0,
      "learning_rate": 2.6998535878030584e-08,
      "loss": 2.9531,
      "step": 295
    },
    {
      "epoch": 0.9579288025889967,
      "grad_norm": 338.0,
      "learning_rate": 2.3285134909173113e-08,
      "loss": 2.7656,
      "step": 296
    },
    {
      "epoch": 0.9611650485436893,
      "grad_norm": 296.0,
      "learning_rate": 1.9845153376933102e-08,
      "loss": 2.0,
      "step": 297
    },
    {
      "epoch": 0.9644012944983819,
      "grad_norm": 322.0,
      "learning_rate": 1.6678971041542702e-08,
      "loss": 2.5469,
      "step": 298
    },
    {
      "epoch": 0.9676375404530745,
      "grad_norm": 278.0,
      "learning_rate": 1.3786937436895686e-08,
      "loss": 1.8359,
      "step": 299
    },
    {
      "epoch": 0.970873786407767,
      "grad_norm": 300.0,
      "learning_rate": 1.1169371831959986e-08,
      "loss": 2.4062,
      "step": 300
    },
    {
      "epoch": 0.9741100323624595,
      "grad_norm": 246.0,
      "learning_rate": 8.826563195531713e-09,
      "loss": 1.3672,
      "step": 301
    },
    {
      "epoch": 0.9773462783171522,
      "grad_norm": 292.0,
      "learning_rate": 6.758770164334572e-09,
      "loss": 1.6406,
      "step": 302
    },
    {
      "epoch": 0.9805825242718447,
      "grad_norm": 312.0,
      "learning_rate": 4.96622101446631e-09,
      "loss": 2.8281,
      "step": 303
    },
    {
      "epoch": 0.9838187702265372,
      "grad_norm": 340.0,
      "learning_rate": 3.449113636199153e-09,
      "loss": 2.2031,
      "step": 304
    },
    {
      "epoch": 0.9870550161812298,
      "grad_norm": 326.0,
      "learning_rate": 2.2076155121328326e-09,
      "loss": 3.4844,
      "step": 305
    },
    {
      "epoch": 0.9902912621359223,
      "grad_norm": 278.0,
      "learning_rate": 1.2418636987057697e-09,
      "loss": 1.8672,
      "step": 306
    },
    {
      "epoch": 0.9935275080906149,
      "grad_norm": 71.0,
      "learning_rate": 5.519648110638431e-10,
      "loss": 0.2266,
      "step": 307
    },
    {
      "epoch": 0.9967637540453075,
      "grad_norm": 318.0,
      "learning_rate": 1.3799501129063698e-10,
      "loss": 3.4688,
      "step": 308
    },
    {
      "epoch": 1.0,
      "grad_norm": 294.0,
      "learning_rate": 0.0,
      "loss": 2.0469,
      "step": 309
    },
    {
      "epoch": 1.0,
      "eval_runtime": 8391.7287,
      "eval_samples_per_second": 0.009,
      "eval_steps_per_second": 0.009,
      "step": 309
    },
    {
      "epoch": 1.0,
      "step": 309,
      "total_flos": 9.996236790693888e+16,
      "train_loss": 2.447854248836974,
      "train_runtime": 38101.0404,
      "train_samples_per_second": 0.008,
      "train_steps_per_second": 0.008
    },
    {
      "epoch": 1.0,
      "eval_runtime": 7046.6433,
      "eval_samples_per_second": 0.01,
      "eval_steps_per_second": 0.01,
      "step": 309
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 309,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9.996236790693888e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
